{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aStgWSO0E0E"
      },
      "source": [
        "# **Data Collection**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eLEkw5O0ECa"
      },
      "source": [
        "## Objectives\n",
        "\n",
        "* Download data from Kaggle.com and perform an initial EDA.\n",
        "\n",
        "## Inputs\n",
        "\n",
        "* unclean_smartwatch_health_data.csv\n",
        "\n",
        "## Outputs\n",
        "\n",
        "* ydata-profiling EDA\n",
        "\n",
        "## Additional Comments\n",
        "\n",
        "* In case you have any additional comments that don't fit in the previous bullets, please state them here. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uWZXH9LwoQg"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqP-UeN-z3i2"
      },
      "source": [
        "# Change working directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* We are assuming you will store the notebooks in a subfolder, therefore when running the notebook in the editor, you will need to change the working directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOGIGS-uz3i2"
      },
      "source": [
        "We need to change the working directory from its current folder to its parent folder\n",
        "* We access the current directory with os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZfF_j-Bz3i4",
        "outputId": "66943449-1436-4c3d-85c7-b85f9f78349b"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MWW8E7lz3i7"
      },
      "source": [
        "We want to make the parent of the current directory the new current directory\n",
        "* os.path.dirname() gets the parent directory\n",
        "* os.chir() defines the new current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TwHsQRWjz3i9",
        "outputId": "86849db3-cd2f-4cc5-ebb8-2d0caafa1a2c"
      },
      "outputs": [],
      "source": [
        "os.chdir(os.path.dirname(current_dir))\n",
        "print(\"You set a new current directory\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_xPk_Ijz3i-"
      },
      "source": [
        "Confirm the new current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vz3S-_kjz3jA",
        "outputId": "00b79ae4-75d0-4a96-d193-ac9ef9847ea2"
      },
      "outputs": [],
      "source": [
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Include data path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "DataUntouched = \"inputs/smartwatch_health_data_untouched\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "data = pd.read_csv(DataUntouched + \"/unclean_smartwatch_health_data.csv\")\n",
        "df = pd.DataFrame(data)\n",
        "print(df.head())\n",
        "\n",
        "# Change version variable to store outputs in different folder\n",
        "version = \"v1\"\n",
        "\n",
        "OutputFolder = f\"outputs/{version}/\"\n",
        "if \"outputs\" in os.listdir(current_dir):\n",
        "    if version not in os.listdir(current_dir + \"/outputs\"):\n",
        "        os.mkdir(OutputFolder)\n",
        "else:\n",
        "    os.makedirs(OutputFolder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mavJ8DibrcQ"
      },
      "source": [
        "# Clean Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Cleaning will be performed, as from the initial EDA we can see we have 1551 missing cells across all features, Hypothesis 1 doesnt have a target variable as we are looking to perform unsupervised clustering to group. Hypothesis 2's target is Stress Levels and hypothesis 3's is Step Count.\n",
        "\n",
        "Now we will drop the User ID feature and perform imputation of numeric and categoric variables.\n",
        "Also try and imporove normality and skewness on the columns that require it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZY3l0-AxO93d"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Drop User ID from a copy of the original dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_todrop = df.copy()\n",
        "df_dropped = df_todrop.drop(\"User ID\", axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(df_dropped.columns)\n",
        "df_dropped.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Before imputing im going to try improving the normal distribution of the variables, as this decides wether you use the median or the mean for imputation of numerics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from feature_engine import transformation as vt\n",
        "from feature_engine.imputation import MeanMedianImputer\n",
        "import seaborn as sns\n",
        "import pingouin as pg\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "df_numeric = df_dropped.select_dtypes(include=['float64','int64'])\n",
        "df_numeric.head()\n",
        "\n",
        "def calculate_skew_kurtosis(df,col, moment):\n",
        "  print(f\"{moment}  | skewness: {df[col].skew().round(2)} | kurtosis: {df[col].kurtosis().round(2)}\")\n",
        "\n",
        "\n",
        "# We set the pipeline with this transformer: vt.BoxCoxTransformer().\n",
        "# Then we .fit_transform() the pipeline, assigning the result to df_transformed\n",
        "\n",
        "pipeline = Pipeline([\n",
        "      ('median',  MeanMedianImputer(imputation_method='median') ),\n",
        "      ( 'log', vt.BoxCoxTransformer() ) # Main difference here\n",
        "  ])\n",
        "\n",
        "df_transformed = pipeline.fit_transform(df_numeric)\n",
        "print(df_transformed.head())\n",
        "\n",
        "def compare_distributions_before_and_after_applying_transformer(df, df_transformed, method):\n",
        "\n",
        "  for col in df.columns:\n",
        "    print(f\"*** {col} ***\")\n",
        "    fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(10,8))\n",
        "\n",
        "    sns.histplot(data=df, x=col, kde=True, ax=axes[0,0])\n",
        "    axes[0,0].set_title(f'Before {method}')\n",
        "    pg.qqplot(df[col], dist='norm',ax=axes[0,1])\n",
        "    \n",
        "    sns.histplot(data=df_transformed, x=col, kde=True, ax=axes[1,0])\n",
        "    axes[1,0].set_title(f'After {method}')\n",
        "    pg.qqplot(df_transformed[col], dist='norm',ax=axes[1,1])\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    calculate_skew_kurtosis(df,col, moment='before transformation')\n",
        "    calculate_skew_kurtosis(df_transformed,col, moment='after transformation')\n",
        "    print(\"\\n\")\n",
        "    \n",
        "compare_distributions_before_and_after_applying_transformer(df_numeric, df_transformed, method='BoxCoxTransformer')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFQo3ycuO-v6"
      },
      "source": [
        "# Section 2 Adjust data types\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Change datatypes of these columns after datasets missing values are imputed\n",
        "df[\"Sleep Duration (hours)\"].astype(\"float64\")\n",
        "df[\"Stress Level\"].astype(\"int64\")\n",
        "\n",
        "# Check the data types of the columns again\n",
        "print(\"Data types of columns:\")\n",
        "for column, dtype in df.dtypes.items():\n",
        "    print(f\"{column}: {dtype}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Section 2 content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "NOTE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* You may add as many sections as you want, as long as it supports your project workflow.\n",
        "* All notebook's cells should be run top-down (you can't create a dynamic wherein a given point you need to go back to a previous cell to execute some task, like go back to a previous cell and refresh a variable content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltNetd085qHf"
      },
      "source": [
        "# Push files to Repo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* In case you don't need to push files to Repo, you may replace this section with \"Conclusions and Next Steps\" and state your conclusions and next steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aKlnIozA4eQO",
        "outputId": "fd09bc1f-adb1-4511-f6ce-492a6af570c0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "try:\n",
        "  # create here your folder\n",
        "  # os.makedirs(name='')\n",
        "except Exception as e:\n",
        "  print(e)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Data Practitioner Jupyter Notebook.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    },
    "orig_nbformat": 2
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
