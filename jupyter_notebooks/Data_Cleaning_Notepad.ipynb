{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aStgWSO0E0E"
      },
      "source": [
        "# **Data Collection**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eLEkw5O0ECa"
      },
      "source": [
        "## Objectives\n",
        "\n",
        "* Download data from Kaggle.com and perform an initial EDA.\n",
        "\n",
        "## Inputs\n",
        "\n",
        "* unclean_smartwatch_health_data.csv\n",
        "\n",
        "## Outputs\n",
        "\n",
        "* ydata-profiling EDA\n",
        "\n",
        "## Additional Comments\n",
        "\n",
        "* In case you have any additional comments that don't fit in the previous bullets, please state them here. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uWZXH9LwoQg"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqP-UeN-z3i2"
      },
      "source": [
        "# Change working directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* We are assuming you will store the notebooks in a subfolder, therefore when running the notebook in the editor, you will need to change the working directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOGIGS-uz3i2"
      },
      "source": [
        "We need to change the working directory from its current folder to its parent folder\n",
        "* We access the current directory with os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "wZfF_j-Bz3i4",
        "outputId": "66943449-1436-4c3d-85c7-b85f9f78349b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'/workspaces/Predictive_Analytics_Project/jupyter_notebooks'"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MWW8E7lz3i7"
      },
      "source": [
        "We want to make the parent of the current directory the new current directory\n",
        "* os.path.dirname() gets the parent directory\n",
        "* os.chir() defines the new current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "TwHsQRWjz3i9",
        "outputId": "86849db3-cd2f-4cc5-ebb8-2d0caafa1a2c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You set a new current directory\n"
          ]
        }
      ],
      "source": [
        "os.chdir(os.path.dirname(current_dir))\n",
        "print(\"You set a new current directory\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_xPk_Ijz3i-"
      },
      "source": [
        "Confirm the new current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "vz3S-_kjz3jA",
        "outputId": "00b79ae4-75d0-4a96-d193-ac9ef9847ea2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'/workspaces/Predictive_Analytics_Project'"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Include data path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "DataUntouched = \"inputs/smartwatch_health_data_untouched\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   User ID  Heart Rate (BPM)  Blood Oxygen Level (%)    Step Count  \\\n",
            "0   4174.0         58.939776               98.809650   5450.390578   \n",
            "1      NaN               NaN               98.532195    727.601610   \n",
            "2   1860.0        247.803052               97.052954   2826.521994   \n",
            "3   2294.0         40.000000               96.894213  13797.338044   \n",
            "4   2130.0         61.950165               98.583797  15679.067648   \n",
            "\n",
            "  Sleep Duration (hours) Activity Level Stress Level  \n",
            "0      7.167235622316564  Highly Active            1  \n",
            "1      6.538239375570314  Highly_Active            5  \n",
            "2                  ERROR  Highly Active            5  \n",
            "3      7.367789630207228          Actve            3  \n",
            "4                    NaN  Highly_Active            6  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "data = pd.read_csv(DataUntouched + \"/unclean_smartwatch_health_data.csv\")\n",
        "df = pd.DataFrame(data)\n",
        "print(df.head())\n",
        "\n",
        "# Change version variable to store outputs in different folder\n",
        "version = \"v1\"\n",
        "\n",
        "OutputFolder = f\"outputs/{version}/\"\n",
        "if \"outputs\" in os.listdir(current_dir):\n",
        "    if version not in os.listdir(current_dir + \"/outputs\"):\n",
        "        os.mkdir(OutputFolder)\n",
        "else:\n",
        "    os.makedirs(OutputFolder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mavJ8DibrcQ"
      },
      "source": [
        "# Clean Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Cleaning will be performed, as from the initial EDA we can see we have 1551 missing cells across all features, Hypothesis 1 doesnt have a target variable as we are looking to perform unsupervised clustering to group. Hypothesis 2's target is Stress Levels and hypothesis 3's is Step Count.\n",
        "\n",
        "Now we will drop the User ID feature and perform imputation of numeric and categoric variables.\n",
        "Also try and imporove normality and skewness on the columns that require it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZY3l0-AxO93d"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Drop User ID from a copy of the original dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_todrop = df.copy()\n",
        "df_dropped = df_todrop.drop(\"User ID\", axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Index(['Heart Rate (BPM)', 'Blood Oxygen Level (%)', 'Step Count',\n",
            "       'Sleep Duration (hours)', 'Activity Level', 'Stress Level'],\n",
            "      dtype='object')\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Heart Rate (BPM)          400\n",
              "Blood Oxygen Level (%)    300\n",
              "Step Count                100\n",
              "Sleep Duration (hours)    150\n",
              "Activity Level            200\n",
              "Stress Level              200\n",
              "dtype: int64"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(df_dropped.columns)\n",
        "df_dropped.head()\n",
        "df_dropped.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now lets check the initial distribution and general analysis of the one true categorical column Acitivity Level"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array(['Highly Active', 'Highly_Active', 'Actve', 'Seddentary',\n",
              "       'Sedentary', 'Active', nan], dtype=object)"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_category_test = df_dropped[\"Activity Level\"]\n",
        "df_category_test.unique()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We note the categorical column has 5 unique classes, not including the NaN value"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note that encoding of this categorical column seems likely. The distributions of each class in Actitivity level are well balanced. There should be low bias with this metric\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Activity Level\n",
              "Seddentary       1676\n",
              "Sedentary        1657\n",
              "Highly Active    1650\n",
              "Active           1643\n",
              "Actve            1622\n",
              "Highly_Active    1552\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_category_test.value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lets take a closer look at the other two data type objects that should be a float64 and int"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next we note that column Sleep Duration (hours) has some cells that include the string \"ERROR\", we must replace or drop this to impute and change the variables datatype. We could replace this value with NaN, and then impute with the median"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Sleep Duration (hours)\n",
              "ERROR                 247\n",
              "4.515512633313341       1\n",
              "6.192069563693488       1\n",
              "8.225011860105145       1\n",
              "7.77547280382428        1\n",
              "                     ... \n",
              "7.809611926858791       1\n",
              "6.5424774602354105      1\n",
              "5.690109349564968       1\n",
              "7.144720940526833       1\n",
              "9.572659844239388       1\n",
              "Name: count, Length: 9604, dtype: int64"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_dropped[\"Sleep Duration (hours)\"].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We also note that column Stress Level includes numbers 1 to 10, and one variable named \"Very High\", we may replace very high with 11 or a custom value based on dataset knowledge"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Stress Level\n",
              "2            1007\n",
              "7            1006\n",
              "6            1001\n",
              "3             995\n",
              "1             984\n",
              "9             976\n",
              "4             966\n",
              "10            954\n",
              "5             945\n",
              "8             917\n",
              "Very High      49\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_dropped[\"Stress Level\"].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lets go ahead replace the \"Very High\" class in the Stress Level column with 11"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Stress Level\n",
              "2     1007\n",
              "7     1006\n",
              "6     1001\n",
              "3      995\n",
              "1      984\n",
              "9      976\n",
              "4      966\n",
              "10     954\n",
              "5      945\n",
              "8      917\n",
              "11      49\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_preprocess = df_dropped.copy()\n",
        "\n",
        "df_preprocess[\"Stress Level\"] = df_preprocess[\"Stress Level\"].replace(\"Very High\", 11)\n",
        "df_preprocess[\"Stress Level\"].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lets replace ERROR cells in the Sleep Duration column with NaN. But Instead of replacing all non-numeric values with NaN we can just use the coerce parameter in pd.to_numeric below, to change any non-numeric values into NaN."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now lets attempt changing the dtype on the two columns Sleep Duration and Stress Level"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data types after conversion to numeric:\n",
            "Heart Rate (BPM)          float64\n",
            "Blood Oxygen Level (%)    float64\n",
            "Step Count                float64\n",
            "Sleep Duration (hours)    float64\n",
            "Activity Level             object\n",
            "Stress Level              float64\n",
            "dtype: object\n"
          ]
        }
      ],
      "source": [
        "# Convert the columns to numeric data types, handling non-numeric values\n",
        "df_preprocess[\"Sleep Duration (hours)\"] = pd.to_numeric(df_preprocess[\"Sleep Duration (hours)\"], errors='coerce')\n",
        "df_preprocess[\"Stress Level\"] = pd.to_numeric(df_preprocess[\"Stress Level\"], errors='coerce')\n",
        "\n",
        "# Check the data types of the columns after conversion\n",
        "print(\"Data types after conversion to numeric:\")\n",
        "print(df_preprocess.dtypes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "from feature_engine.imputation import MeanMedianImputer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Define the pipeline with MeanMedianImputer\n",
        "pipeline = Pipeline([\n",
        "    ('median', MeanMedianImputer(imputation_method='median',\n",
        "                                 variables=[\"Sleep Duration (hours)\", \"Stress Level\"]))\n",
        "])\n",
        "\n",
        "# Fit and transform the data using the pipeline\n",
        "df_processed = pipeline.fit_transform(df_preprocess)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now lets attempt changing the dtype on the Stress Level as to_numeric made it a float."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data types after setting explicit types:\n",
            "Heart Rate (BPM)          float64\n",
            "Blood Oxygen Level (%)    float64\n",
            "Step Count                float64\n",
            "Sleep Duration (hours)    float64\n",
            "Activity Level             object\n",
            "Stress Level                int64\n",
            "dtype: object\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Heart Rate (BPM)</th>\n",
              "      <th>Blood Oxygen Level (%)</th>\n",
              "      <th>Step Count</th>\n",
              "      <th>Sleep Duration (hours)</th>\n",
              "      <th>Activity Level</th>\n",
              "      <th>Stress Level</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>58.939776</td>\n",
              "      <td>98.809650</td>\n",
              "      <td>5450.390578</td>\n",
              "      <td>7.167236</td>\n",
              "      <td>Highly Active</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>NaN</td>\n",
              "      <td>98.532195</td>\n",
              "      <td>727.601610</td>\n",
              "      <td>6.538239</td>\n",
              "      <td>Highly_Active</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>247.803052</td>\n",
              "      <td>97.052954</td>\n",
              "      <td>2826.521994</td>\n",
              "      <td>6.503308</td>\n",
              "      <td>Highly Active</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>40.000000</td>\n",
              "      <td>96.894213</td>\n",
              "      <td>13797.338044</td>\n",
              "      <td>7.367790</td>\n",
              "      <td>Actve</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>61.950165</td>\n",
              "      <td>98.583797</td>\n",
              "      <td>15679.067648</td>\n",
              "      <td>6.503308</td>\n",
              "      <td>Highly_Active</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Heart Rate (BPM)  Blood Oxygen Level (%)    Step Count  \\\n",
              "0         58.939776               98.809650   5450.390578   \n",
              "1               NaN               98.532195    727.601610   \n",
              "2        247.803052               97.052954   2826.521994   \n",
              "3         40.000000               96.894213  13797.338044   \n",
              "4         61.950165               98.583797  15679.067648   \n",
              "\n",
              "   Sleep Duration (hours) Activity Level  Stress Level  \n",
              "0                7.167236  Highly Active             1  \n",
              "1                6.538239  Highly_Active             5  \n",
              "2                6.503308  Highly Active             5  \n",
              "3                7.367790          Actve             3  \n",
              "4                6.503308  Highly_Active             6  "
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Set explicit data types\n",
        "df_processed[\"Stress Level\"] = df_processed[\"Stress Level\"].astype(\"int64\")\n",
        "\n",
        "# Check the data types of the columns again\n",
        "print(\"Data types after setting explicit types:\")\n",
        "print(df_processed.dtypes)\n",
        "df_processed.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Skewness and Kurtosis before transformation and imputation:\n",
            "Sleep Duration (hours) | Skewness: 0.006315730832954415 | Kurtosis: -0.06651157982483946\n",
            "Stress Level | Skewness: 0.02130785311919279 | Kurtosis: -1.2141975459496481\n",
            "Skewness and Kurtosis after transformation and imputation:\n",
            "Sleep Duration (hours) | Skewness: 0.006618365704973535 | Kurtosis: 0.05477632157071133\n",
            "Stress Level | Skewness: 0.010961089966155042 | Kurtosis: -1.180198079757423\n"
          ]
        }
      ],
      "source": [
        "# Check skewness and kurtosis after transformation and imputation\n",
        "print(\"Skewness and Kurtosis before transformation and imputation:\")\n",
        "for col in [\"Sleep Duration (hours)\", \"Stress Level\"]:\n",
        "    print(f\"{col} | Skewness: {df_preprocess[col].skew()} | Kurtosis: {df_preprocess[col].kurtosis()}\")\n",
        "\n",
        "print(\"Skewness and Kurtosis after transformation and imputation:\")\n",
        "for col in [\"Sleep Duration (hours)\", \"Stress Level\"]:\n",
        "    print(f\"{col} | Skewness: {df_processed[col].skew()} | Kurtosis: {df_processed[col].kurtosis()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The skewness values were already very close to zero before transformation and imputation, indicating that the distributions were already quite symmetrical. The changes in skewness are minimal and do not indicate a significant change.\n",
        "The kurtosis values show slight changes towards a more normal distribution, but the improvements are not substantial."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now the dataset has the correct data types"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFQo3ycuO-v6"
      },
      "source": [
        "# Section 2 Normality, Skewness and Kurtosis Improvement\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Before imputing the rest of the data, im going to try improving the normal distribution of the variables, as this decides wether you use the median or the mean for imputation of numerics. I can now test all numerical columns together after fixing the data types.\n",
        "\n",
        "Will do QQ plots with a BoxCox transformer with before and afters to see what the normality is like for the 3 numeric data types we can test, and then possibly improve skewness, kurtosis and normality with a boxcox with the aim to impute the most variables we can with the mean."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# check min and max for numeric variables to see if boxcox is suitable\n",
        "for col in df_processed\n",
        "    if df_processed[col].dtype == \"float64\" or df_processed[col].dtype == \"int64\":\n",
        "        print(f\"{col} min: {df_processed[col].min()}, max: {df_processed[col].max()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from feature_engine import transformation as vt\n",
        "from feature_engine.imputation import MeanMedianImputer\n",
        "import seaborn as sns\n",
        "import pingouin as pg\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "df_numeric = df_processed.select_dtypes(include=['float64','int64'])\n",
        "df_numeric.head()\n",
        "\n",
        "def calculate_skew_kurtosis(df,col, moment):\n",
        "  print(f\"{moment}  | skewness: {df[col].skew().round(2)} | kurtosis: {df[col].kurtosis().round(2)}\")\n",
        "\n",
        "\n",
        "# We set the pipeline with this transformer: vt.BoxCoxTransformer().\n",
        "# Then we .fit_transform() the pipeline, assigning the result to df_transformed\n",
        "\n",
        "pipeline = Pipeline([\n",
        "      ( 'log', vt.BoxCoxTransformer() ) # Main difference here\n",
        "  ])\n",
        "\n",
        "df_transformed = pipeline.fit_transform(df_numeric)\n",
        "print(df_transformed.head())\n",
        "\n",
        "def compare_distributions_before_and_after_applying_transformer(df, df_transformed, method):\n",
        "\n",
        "  for col in df.columns:\n",
        "    print(f\"*** {col} ***\")\n",
        "    fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(10,8))\n",
        "\n",
        "    sns.histplot(data=df, x=col, kde=True, ax=axes[0,0])\n",
        "    axes[0,0].set_title(f'Before {method}')\n",
        "    pg.qqplot(df[col], dist='norm',ax=axes[0,1])\n",
        "    \n",
        "    sns.histplot(data=df_transformed, x=col, kde=True, ax=axes[1,0])\n",
        "    axes[1,0].set_title(f'After {method}')\n",
        "    pg.qqplot(df_transformed[col], dist='norm',ax=axes[1,1])\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Save plot\n",
        "    plot_names = method + col + \".png\"\n",
        "    # Add a subfolder to the output folder for normality and skewness improvement plots\n",
        "    NormalitySkewness = OutputFolder + \"norm_skew_improvement/\"\n",
        "    if \"norm_skew_improvement\" not in os.listdir(OutputFolder):\n",
        "        os.mkdir(NormalitySkewness)\n",
        "    plot_dir = os.path.join(NormalitySkewness, plot_names)\n",
        "    fig.savefig(plot_dir)\n",
        "\n",
        "    calculate_skew_kurtosis(df,col, moment='before transformation')\n",
        "    calculate_skew_kurtosis(df_transformed,col, moment='after transformation')\n",
        "    print(\"\\n\")\n",
        "    \n",
        "compare_distributions_before_and_after_applying_transformer(df_numeric, df_transformed, method='BoxCoxTransformer')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Section 2 content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "NOTE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* You may add as many sections as you want, as long as it supports your project workflow.\n",
        "* All notebook's cells should be run top-down (you can't create a dynamic wherein a given point you need to go back to a previous cell to execute some task, like go back to a previous cell and refresh a variable content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltNetd085qHf"
      },
      "source": [
        "# Push files to Repo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* In case you don't need to push files to Repo, you may replace this section with \"Conclusions and Next Steps\" and state your conclusions and next steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aKlnIozA4eQO",
        "outputId": "fd09bc1f-adb1-4511-f6ce-492a6af570c0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "try:\n",
        "  # create here your folder\n",
        "  # os.makedirs(name='')\n",
        "except Exception as e:\n",
        "  print(e)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Data Practitioner Jupyter Notebook.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    },
    "orig_nbformat": 2
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
